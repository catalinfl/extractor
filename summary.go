package main

/*
SISTEM DE REZUMATE MULTI-NIVEL

DIFEREN»öE IMPORTANTE:

1. REZUMAT GENERAL:
   - Prime»ôte TOT textul PDF-ului
   - AnalizeazƒÉ √Æntregul document pentru temƒÉ centralƒÉ
   - Foarte concis (3-4 propozi»õii)

2. REZUMAT PE CAPITOLE:
   - Prime»ôte TOT textul PDF-ului
   - DetecteazƒÉ capitole/sec»õiuni »ôi le analizeazƒÉ individual
   - Moderat (5-8 propozi»õii per capitol)

3. REZUMATE PE NIVELURI (1-10):
   - LucreazƒÉ cu CHUNK-URI de pagini
   - Fiecare chunk este procesat separat
   - Nivel 1: 3 pagini/chunk (foarte general)
   - Nivel 10: 20 pagini/chunk (foarte detaliat)
   - Pentru fiecare nivel se combinƒÉ rezumatele chunk-urilor

FLUX:
- Extract PDF ‚Üí Text complet
- Rezumat general ‚Üê Text complet
- Rezumat capitole ‚Üê Text complet
- Rezumate niveluri ‚Üê Chunk-uri de text
*/

import (
	"encoding/json"
	"fmt"
	"math"
	"os"
	"strings"
	"sync"
	"time"
)

// SummaryLevel reprezintƒÉ un nivel de rezumat
type SummaryLevel struct {
	Level         int    `json:"level"`
	Description   string `json:"description"`
	PagesPerChunk int    `json:"pages_per_chunk"`
	Summary       string `json:"summary"`
}

// SummaryResult reprezintƒÉ rezultatul complet al rezumƒÉrii
type SummaryResult struct {
	OriginalPages  int            `json:"original_pages"`
	GeneralSummary string         `json:"general_summary"`
	ChapterSummary []ChapterInfo  `json:"chapter_summary,omitempty"`
	Levels         []SummaryLevel `json:"levels"`
	GeneratedAt    time.Time      `json:"generated_at"`
	ProcessingTime string         `json:"processing_time"`
}

// ChapterInfo reprezintƒÉ informa»õii despre un capitol
type ChapterInfo struct {
	Number  int    `json:"number"`
	Title   string `json:"title"`
	Pages   string `json:"pages"`
	Summary string `json:"summary"`
}

// SummaryRequest reprezintƒÉ cererea pentru generarea rezumatului
type SummaryRequest struct {
	Text            string `json:"text"`
	TotalPages      int    `json:"total_pages"`
	Language        string `json:"language,omitempty"`
	IncludeChapters bool   `json:"include_chapters,omitempty"`
	DesiredLevel    int    `json:"desired_level,omitempty"` // 1..10, if 0 -> all levels
}

// calculateSummaryLevels calculeazƒÉ configurarea pentru fiecare nivel
func calculateSummaryLevels(totalPages int, desiredLevel int) SummaryLevel {
	// Clamp desiredLevel to maximum 4
	if desiredLevel <= 0 {
		desiredLevel = 1
	}
	if desiredLevel > 4 {
		desiredLevel = 4
	}

	makeLevel := func(level int) SummaryLevel {
		var pagesPerChunk int

		// Strategii diferite √Æn func»õie de numƒÉrul total de pagini (pƒÉstrƒÉm comportamentul pentru nivelele 1..4)
		if totalPages <= 20 {
			switch level {
			case 1:
				pagesPerChunk = int(math.Max(1, float64(totalPages)/2))
			case 4:
				pagesPerChunk = int(math.Max(1, float64(totalPages)/4))
			default:
				ratio := float64(level-1) / 3
				pagesPerChunk = int(math.Max(1, float64(totalPages)*(0.1+ratio*0.4)))
			}
		} else if totalPages <= 100 {
			switch level {
			case 1:
				pagesPerChunk = int(math.Max(3, float64(totalPages)/3))
			case 4:
				pagesPerChunk = int(math.Max(3, float64(totalPages)/8))
			default:
				chunksTarget := 3 + (level - 1)
				pagesPerChunk = int(math.Max(2, float64(totalPages)/float64(chunksTarget)))
			}
		} else {
			switch level {
			case 1:
				pagesPerChunk = int(math.Max(5, float64(totalPages)/5))
			case 2:
				pagesPerChunk = int(math.Max(4, float64(totalPages)/8))
			case 3:
				pagesPerChunk = int(math.Max(3, float64(totalPages)/12))
			case 4:
				pagesPerChunk = int(math.Max(3, float64(totalPages)/15))
			}
		}

		if pagesPerChunk > totalPages {
			pagesPerChunk = totalPages
		}

		estimatedChunks := int(math.Ceil(float64(totalPages) / float64(pagesPerChunk)))

		return SummaryLevel{
			Level:         level,
			Description:   fmt.Sprintf("Rezumat nivel %d (%d pagini per chunk, ~%d chunks)", level, pagesPerChunk, estimatedChunks),
			PagesPerChunk: pagesPerChunk,
		}
	}

	return makeLevel(desiredLevel)
}

// chunkTextByPages √Æmparte textul √Æn chunk-uri bazate pe numƒÉrul de pagini
func chunkTextByPages(text string, totalPages int, pagesPerChunk int) []string {
	startTime := time.Now()

	if totalPages <= 0 || pagesPerChunk <= 0 {
		fmt.Printf("‚è±Ô∏è chunkTextByPages: Invalid params, returning single chunk (took: %v)\n", time.Since(startTime))
		return []string{text}
	}

	// EstimeazƒÉ lungimea medie per paginƒÉ
	avgCharsPerPage := len(text) / totalPages
	chunkSize := avgCharsPerPage * pagesPerChunk

	fmt.Printf("üìä chunkTextByPages: text=%d chars, pages=%d, pagesPerChunk=%d, avgCharsPerPage=%d, chunkSize=%d\n",
		len(text), totalPages, pagesPerChunk, avgCharsPerPage, chunkSize)

	if chunkSize >= len(text) {
		fmt.Printf("‚è±Ô∏è chunkTextByPages: Single chunk needed (took: %v)\n", time.Since(startTime))
		return []string{text}
	}

	var chunks []string
	for i := 0; i < len(text); i += chunkSize {
		end := i + chunkSize
		if end > len(text) {
			end = len(text)
		}

		chunk := text[i:end]

		// Optimized sentence boundary detection - limit search to last 500 chars to avoid performance issues
		if end < len(text) {
			searchStart := len(chunk) - 500
			if searchStart < len(chunk)/2 {
				searchStart = len(chunk) / 2
			}

			searchChunk := chunk[searchStart:]
			lastDot := strings.LastIndex(searchChunk, ".")
			lastQuestion := strings.LastIndex(searchChunk, "?")
			lastExclamation := strings.LastIndex(searchChunk, "!")

			lastSentenceEnd := int(math.Max(float64(lastDot), math.Max(float64(lastQuestion), float64(lastExclamation))))
			if lastSentenceEnd > 0 {
				actualEnd := searchStart + lastSentenceEnd + 1
				chunk = chunk[:actualEnd]
				i = i + actualEnd - chunkSize // adjust index
			}
		}

		chunks = append(chunks, strings.TrimSpace(chunk))
	}

	duration := time.Since(startTime)
	fmt.Printf("‚è±Ô∏è chunkTextByPages: Created %d chunks in %v\n", len(chunks), duration)

	return chunks
}

func generateChunkSummary(chunk string, chunkIndex int, totalChunks int, language string) (string, error) {
	startTime := time.Now()

	fmt.Printf("‚è±Ô∏è [Chunk %d/%d] Starting chunk summary generation (%d chars)...\n", chunkIndex+1, totalChunks, len(chunk))

	apiKey := os.Getenv("OPENROUTER_API_KEY")
	if apiKey == "" {
		return "", fmt.Errorf("OPENROUTER_API_KEY environment variable not set")
	}

	prompt := fmt.Sprintf(`E»ôti un expert √Æn rezumarea textelor. FƒÉ un rezumat profesional al acestui CHUNK de text.

ATEN»öIE: Prime»ôti o PARTE (chunk %d din %d) dintr-un document mai mare pentru rezumat!

INFORMA»öII CONTEXT:
- Acesta este chunk-ul %d din %d pentru rezumat
- Limba: %s FOARTE FOARTE IMPORTANT!

INSTRUC»öIUNI IMPORTANTE:
- Acesta este UN FRAGMENT din rezumatul complet
- Rezumatul tƒÉu va fi UNIT direct cu alte chunk-uri (FƒÇRƒÇ procesare suplimentarƒÉ)
- Incearca sa-l faci cat mai detaliat
- Scrie fluent »ôi coerent pentru cƒÉ va fi unit cu alte pƒÉr»õi
- NU folosi introduceri ca "√én acest fragment..." sau "AceastƒÉ parte..."
- √éncepe direct cu con»õinutul relevant

LIMBA: %s FOARTE FOARTE IMPORTANT!

TEXT CHUNK:
%s`, chunkIndex+1, totalChunks, chunkIndex+1, totalChunks, language, language, chunk)

	reqBody := OpenRouterRequest{
		Model:       OpenRouterModel,
		Temperature: 0.3,
		MaxTokens:   2000,
		Messages: []OpenRouterMessage{
			{
				Role:    "system",
				Content: fmt.Sprintf("E»ôti un expert √Æn rezumarea textelor √Æn limba %s. Faci rezumate profesionale »ôi clare.", language),
			},
			{
				Role:    "user",
				Content: prompt,
			},
		},
	}

	startCall := time.Now()
	summary, err := callOpenRouter(reqBody, apiKey)
	callDuration := time.Since(startCall)
	totalDuration := time.Since(startTime)

	fmt.Printf("‚è±Ô∏è [Chunk %d/%d] OpenRouter call took: %v, total chunk time: %v\n", chunkIndex+1, totalChunks, callDuration, totalDuration)

	if err != nil {
		return "", fmt.Errorf("failed to generate summary for chunk %d: %v", chunkIndex+1, err)
	}

	return strings.TrimSpace(summary), nil
}

// generateLevelSummary genereazƒÉ rezumatul pentru un nivel specific
func generateLevelSummary(text string, totalPages int, level SummaryLevel, language string) (string, error) {
	startTime := time.Now()
	fmt.Printf("üìÑ [LEVEL %d] Starting level summary generation (%d pagini per chunk)...\n", level.Level, level.PagesPerChunk)

	startChunking := time.Now()
	chunks := chunkTextByPages(text, totalPages, level.PagesPerChunk)
	chunkingDuration := time.Since(startChunking)
	fmt.Printf("‚è±Ô∏è [LEVEL %d] Text chunking took: %v, created %d chunks\n", level.Level, chunkingDuration, len(chunks))

	// Process chunks in parallel with concurrency limit to avoid rate limiting
	const maxConcurrency = 200
	semaphore := make(chan struct{}, maxConcurrency)
	var wg sync.WaitGroup
	summaries := make([]string, len(chunks))
	errors := make([]error, len(chunks))

	for i, chunk := range chunks {
		wg.Add(1)
		go func(index int, chunkText string) {
			defer wg.Done()

			// Acquire semaphore
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			fmt.Printf("üìÑ [LEVEL %d] Processing chunk %d/%d (size: %d chars) [PARALLEL]...\n", level.Level, index+1, len(chunks), len(chunkText))

			chunkStart := time.Now()
			summary, err := generateChunkSummary(chunkText, index, len(chunks), language)
			chunkDuration := time.Since(chunkStart)

			if err != nil {
				fmt.Printf("‚ùå [LEVEL %d] Error processing chunk %d: %v (took: %v)\n", level.Level, index+1, err, chunkDuration)
				errors[index] = err
			} else {
				fmt.Printf("‚è±Ô∏è [LEVEL %d] Chunk %d/%d completed in: %v [PARALLEL]\n", level.Level, index+1, len(chunks), chunkDuration)
				summaries[index] = summary
			}
		}(i, chunk)
	}

	// Wait for all chunks to complete
	fmt.Printf("‚è≥ [LEVEL %d] Waiting for all %d chunks to complete...\n", level.Level, len(chunks))
	wg.Wait()

	// Check for errors
	for i, err := range errors {
		if err != nil {
			fmt.Printf("‚ùå [LEVEL %d] Failed at chunk %d: %v\n", level.Level, i+1, err)
			return "", err
		}
	}

	// Reunire directƒÉ a chunk-urilor FƒÇRƒÇ procesare suplimentarƒÉ prin AI
	if len(summaries) == 1 {
		totalDuration := time.Since(startTime)
		fmt.Printf("‚è±Ô∏è [LEVEL %d] Single chunk completed in total: %v\n", level.Level, totalDuration)
		return summaries[0], nil
	}

	// CombinƒÉ chunk-urile direct cu separatori
	startCombining := time.Now()
	fmt.Printf("üìÑ [LEVEL %d] Combining %d chunks directly without AI processing...\n", level.Level, len(summaries))
	finalSummary := strings.Join(summaries, "\n\n")
	combiningDuration := time.Since(startCombining)
	totalDuration := time.Since(startTime)

	fmt.Printf("‚è±Ô∏è [LEVEL %d] Combining took: %v, total level processing: %v\n", level.Level, combiningDuration, totalDuration)

	return finalSummary, nil
}

// generateChapterSummaries generates a list of chapters detected by AI and returns
// a structured JSON: []ChapterInfo. It sends the LLM the entire text (text) and asks it
// to detect the chapters, titles, page ranges (if possible), and a short summary
// for each chapter. The function sanitizes the response and decodes it.
func generateChapterSummaries(text string, language string) ([]ChapterInfo, error) {
	apiKey := os.Getenv("OPENROUTER_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("OPENROUTER_API_KEY environment variable not set")
	}

	// Prompt requires a json response with {number,title,pages,summary}
	prompt := fmt.Sprintf(`E»ôti un asistent care detecteazƒÉ capitolele »ôi sec»õiunile dintr-un document.
ReturneazƒÉ DOAR un ARRAY JSON (√Æncep√¢nd cu '[') cu obiecte av√¢nd exact c√¢mpurile:
 - number (integer) -> numƒÉrul capitolului, √Æn ordine
 - title (string) -> titlul capitolului (dacƒÉ nu are titlu, pune "Capitolul N")
 - pages (string) -> intervalul de pagini sau estimare (ex: "1-10")
 - summary (string) -> rezumat scurt al capitolului (5-8 propozi»õii)

RƒÉspunde STRICT cu JSON, fƒÉrƒÉ text explicativ, fƒÉrƒÉ note, fƒÉrƒÉ markdown.

LIMBA IN CARE RASPUNZI: %s !FOARTE IMPORTANT

TEXT COMPLET PDF (anexeazƒÉ tot textul urmƒÉtor):
%s
`, language, text)

	reqBody := OpenRouterRequest{
		Model:       OpenRouterModel,
		Temperature: 0.3,
		MaxTokens:   4000,
		Messages: []OpenRouterMessage{
			{
				Role:    "system",
				Content: fmt.Sprintf("E»ôti un expert care extrage capitole »ôi rezumate din documente √Æn limba %s. Returnezi doar JSON.", language),
			},
			{
				Role:    "user",
				Content: prompt,
			},
		},
	}

	resp, err := callOpenRouter(reqBody, apiKey)
	if err != nil {
		return nil, fmt.Errorf("failed to call OpenRouter for chapters: %v", err)
	}
	// Normalize response: remove common code fences and stray backticks
	raw := strings.TrimSpace(resp)
	raw = strings.ReplaceAll(raw, "```json", "")
	raw = strings.ReplaceAll(raw, "```", "")
	raw = strings.Trim(raw, "` \n\r\t")
	raw = strings.TrimSpace(raw)

	// Attempt to extract JSON array between first '[' and last ']' or fallback to object between '{' and '}'
	firstArray := strings.Index(raw, "[")
	lastArray := strings.LastIndex(raw, "]")

	var jsonPart string
	if firstArray != -1 && lastArray != -1 && lastArray > firstArray {
		jsonPart = raw[firstArray : lastArray+1]
	} else {
		// fallback: try a single object and wrap into array
		firstObj := strings.Index(raw, "{")
		lastObj := strings.LastIndex(raw, "}")
		if firstObj != -1 && lastObj != -1 && lastObj > firstObj {
			jsonPart = "[" + raw[firstObj:lastObj+1] + "]"
		} else {
			// give up extracting and use the whole cleaned raw
			jsonPart = raw
		}
	}

	// First attempt: sanitize JSON-friendly characters inside strings
	sanitized := sanitizeJSONString(jsonPart)

	var chapters []ChapterInfo
	if err := json.Unmarshal([]byte(sanitized), &chapters); err != nil {
		if err2 := json.Unmarshal([]byte(jsonPart), &chapters); err2 == nil {
		} else {
			orig := strings.TrimSpace(resp)
			orig = strings.ReplaceAll(orig, "```json", "")
			orig = strings.ReplaceAll(orig, "```", "")
			orig = strings.Trim(orig, "` \n\r\t")
			if err3 := json.Unmarshal([]byte(orig), &chapters); err3 == nil {
			} else {
				return nil, fmt.Errorf("failed to decode chapters JSON: %v; response: %s", err, resp)
			}
		}
	}

	for i := range chapters {
		if chapters[i].Number == 0 {
			chapters[i].Number = i + 1
		}
		if strings.TrimSpace(chapters[i].Title) == "" {
			chapters[i].Title = fmt.Sprintf("Capitolul %d", chapters[i].Number)
		}
		if strings.TrimSpace(chapters[i].Pages) == "" {
			chapters[i].Pages = "n/a"
		}
	}

	return chapters, nil
}

// generateGeneralSummary genereazƒÉ un rezumat general foarte scurt - PRIME»òTE TOT TEXTUL PDF
func generateGeneralSummary(text string, language string) (string, error) {
	apiKey := os.Getenv("OPENROUTER_API_KEY")
	if apiKey == "" {
		return "", fmt.Errorf("OPENROUTER_API_KEY environment variable not set")
	}

	var textForSummary string
	textLen := len(text)

	if textLen <= 9000 {
		textForSummary = text
	} else {
		start := text[:3000]
		middle := text[textLen/2-1500 : textLen/2+1500]
		end := text[textLen-3000:]
		textForSummary = start + "\n\n[...mijloc document...]\n\n" + middle + "\n\n[...sf√¢r»ôit document...]\n\n" + end
	}

	prompt := fmt.Sprintf(`AnalizeazƒÉ √éNTREG documentul PDF »ôi fƒÉ un rezumat foarte concis »ôi general. 

ATEN»öIE: Prime»ôti TOT TEXTUL PDF-ului, nu doar un fragment!

TREBUIE SCRIS NEAPARAT √éN LIMBA: %s

Instruc»õiuni pentru REZUMAT GENERAL:
- Doar ideile principale »ôi tema centralƒÉ din √éNTREG documentul
- IdentificƒÉ subiectul principal al √Æntregului PDF
- Stil profesional »ôi clar, incearcƒÉ sƒÉ fie c√¢t mai lung, sƒÉ exprimi c√¢t mai mult
- NU men»õioneazƒÉ cƒÉ este un fragment sau chunk

TEXT COMPLET PDF:
%s`, language, textForSummary)

	reqBody := OpenRouterRequest{
		Model:       OpenRouterModel,
		Temperature: 0.2,
		MaxTokens:   1000,
		Messages: []OpenRouterMessage{
			{
				Role:    "system",
				Content: fmt.Sprintf("E»ôti un expert √Æn rezumarea concisƒÉ de texte √Æn limba %s.", language),
			},
			{
				Role:    "user",
				Content: prompt,
			},
		},
	}

	summary, err := callOpenRouter(reqBody, apiKey)
	if err != nil {
		return "", err
	}

	return strings.TrimSpace(summary), nil
}

// detectLanguageFromText detects the language using AI Request
// func detectLanguageFromText(text string) (string, error) {
// 	apiKey := os.Getenv("OPENROUTER_API_KEY")
// 	if apiKey == "" {
// 		return "english", nil // Default fallback
// 	}

// 	sampleText := text
// 	if len(text) > 500 {
// 		sampleText = text[:500]
// 	}

// 	prompt := fmt.Sprintf(`DetecteazƒÉ limba principalƒÉ din urmƒÉtorul text »ôi returneazƒÉ DOAR numele limbii √Æn englezƒÉ.

// RƒÉspunde cu UNA din urmƒÉtoarele op»õiuni exacte, CA EXEMPLU:
// 	- english
// 	- romanian
// 	- french etc.
// ReturneazƒÉ DOAR numele limbii, fƒÉrƒÉ explica»õii.

// TEXT:
// %s`, sampleText)

// 	reqBody := OpenRouterRequest{
// 		Model:       OpenRouterModel,
// 		Temperature: 0.1,
// 		MaxTokens:   10,
// 		Messages: []OpenRouterMessage{
// 			{
// 				Role:    "system",
// 				Content: "E»ôti un expert √Æn detectarea limbilor. RƒÉspunzi doar cu numele limbii √Æn englezƒÉ.",
// 			},
// 			{
// 				Role:    "user",
// 				Content: prompt,
// 			},
// 		},
// 	}

// 	response, err := callOpenRouter(reqBody, apiKey)
// 	if err != nil {
// 		fmt.Printf("Error at detecting language: %v, using English as default\n", err)
// 		return "english", nil
// 	}

// 	language := strings.ToLower(strings.TrimSpace(response))

// 	fmt.Printf("Detected language: %s\n", language)

// 	return language, nil
// }

/*
NEEDS TESTS, MIGHT BE DEVELOPED IN FUTURE
COULD BE PROCESSED AS MULTIPLE SOLUTION
------------------------------------------

func processSummaryRequest(request SummaryRequest) (*SummaryResult, error) {
	startTime := time.Now()

	fmt.Printf("üìÑ √éncepe procesarea rezumatului pentru %d pagini...\n", request.TotalPages)

	language := request.Language
	if language == "" {
		language = "english"
	}

	result := &SummaryResult{
		OriginalPages: request.TotalPages,
		GeneratedAt:   startTime,
	}

	// 1. GenereazƒÉ rezumatul general (prime»ôte TOT textul PDF)
	fmt.Printf("üìÑ Generez rezumatul general cu TOT textul PDF...\n")
	generalSummary, err := generateGeneralSummary(request.Text, language)
	if err != nil {
		return nil, fmt.Errorf("failed to generate general summary: %v", err)
	}
	result.GeneralSummary = generalSummary

	// 2. GenereazƒÉ rezumate pe capitole (prime»ôte TOT textul PDF)
	if request.IncludeChapters {
		fmt.Printf("üìÑ Generez rezumate pe capitole cu TOT textul PDF...\n")
		chapterSummaries, err := generateChapterSummaries(request.Text, language)
		if err != nil {
			fmt.Printf("‚ö†Ô∏è Eroare la generarea rezumatelor pe capitole: %v\n", err)
		} else {
			result.ChapterSummary = chapterSummaries
		}
	}

	// 3. CalculeazƒÉ nivelul de rezumat selectat (vor lucra cu chunk-uri)
	selectedLevel := calculateSummaryLevels(request.TotalPages, request.DesiredLevel)

	// 4. GenereazƒÉ rezumat pentru nivelul selectat (lucreazƒÉ cu chunk-uri)
	fmt.Printf("üìÑ Generez rezumat pentru nivelul %d (chunk-uri)...\n", selectedLevel.Level)
	summary, err := generateLevelSummary(request.Text, request.TotalPages, selectedLevel, language)
	if err != nil {
		fmt.Printf("‚ö†Ô∏è Eroare la generarea rezumatului la nivelul %d: %v\n", selectedLevel.Level, err)
	} else {
		selectedLevel.Summary = summary
	}

	result.Levels = []SummaryLevel{selectedLevel}
	result.ProcessingTime = time.Since(startTime).String()

	fmt.Printf("‚úÖ Rezumat generat cu succes √Æn %s\n", result.ProcessingTime)
	return result, nil
}
*/
